{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liveability Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the merged CSV\n",
    "csv_path = '../data/curated/sa2_rent/merged_final.csv'\n",
    "\n",
    "# Load the merged DataFrame from the CSV\n",
    "merged_df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df['geometry'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust the column names according to your dataset\n",
    "selected_columns = [\n",
    "    'Suburbs',  # Spatial data identifier\n",
    "    'geometry',  # Geometry for spatial data (ensure this is included)\n",
    "    'MARCH2024COUNT_FLAT1',  # Replace with actual column name for flat 1 counts\n",
    "    'MARCH2024MEDIAN_FLAT1',  # Replace with actual column name for flat 1 medians\n",
    "    'MARCH2024COUNT_FLAT2',  # Replace with actual column name for flat 2 counts\n",
    "    'MARCH2024MEDIAN_FLAT2',  # Replace with actual column name for flat 2 medians\n",
    "    'MARCH2024COUNT_FLAT3',  # Replace with actual column name for flat 3 counts\n",
    "    'MARCH2024MEDIAN_FLAT3',  # Replace with actual column name for flat 3 medians\n",
    "    'MARCH2024COUNT_HOUSE2',  # Replace with actual column name for house 2 counts\n",
    "    'MARCH2024MEDIAN_HOUSE2',  # Replace with actual column name for house 2 medians\n",
    "    'MARCH2024COUNT_HOUSE3',  # Replace with actual column name for house 3 counts\n",
    "    'MARCH2024MEDIAN_HOUSE3',  # Replace with actual column name for house 3 medians\n",
    "    'MARCH2024COUNT_HOUSE4',  # Replace with actual column name for house 4 counts\n",
    "    'MARCH2024MEDIAN_HOUSE4',  # Replace with actual column name for house 4 medians\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming merged_df is your combined DataFrame with suburbs and rent data\n",
    "# Let's make sure you have the correct column names for your plot\n",
    "\n",
    "# Define a function to plot the top 5 and bottom 5 for a specific column\n",
    "def plot_top_bottom(data, column, title, xlabel):\n",
    "    # Ensure the column is numeric\n",
    "    data[column] = pd.to_numeric(data[column], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing or zero values in the column\n",
    "    data = data.dropna(subset=[column])\n",
    "    data = data[data[column] > 0]\n",
    "\n",
    "    # Sort by the column\n",
    "    sorted_data = data[['Suburbs', column]].sort_values(by=column)\n",
    "    \n",
    "    # Get the top 5 and bottom 5\n",
    "    top_5 = sorted_data.tail(5)\n",
    "    bottom_5 = sorted_data.head(5)\n",
    "    \n",
    "    # Plot top 5 and bottom 5\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot bottom 5\n",
    "    bottom_5.plot(kind='barh', x='Suburbs', y=column, ax=axes[0], color='red', legend=False)\n",
    "    axes[0].set_title(f\"Bottom 5 {title}\")\n",
    "    axes[0].set_xlabel(xlabel)\n",
    "    axes[0].set_ylabel(\"Suburbs\")\n",
    "    \n",
    "    # Plot top 5\n",
    "    top_5.plot(kind='barh', x='Suburbs', y=column, ax=axes[1], color='green', legend=False)\n",
    "    axes[1].set_title(f\"Top 5 {title}\")\n",
    "    axes[1].set_xlabel(xlabel)\n",
    "    axes[1].set_ylabel(\"Suburbs\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define the property types and their corresponding median and count columns\n",
    "property_types = {\n",
    "    'All Properties': ('MARCH2024MEDIAN_RENT', 'MARCH2024COUNT_RENT'),\n",
    "    '1 Bedroom Flat': ('MARCH2024MEDIAN_FLAT1', 'MARCH2024COUNT_FLAT1'),\n",
    "    '2 Bedroom Flat': ('MARCH2024MEDIAN_FLAT2', 'MARCH2024COUNT_FLAT2'),\n",
    "    '3 Bedroom Flat': ('MARCH2024MEDIAN_FLAT3', 'MARCH2024COUNT_FLAT3'),\n",
    "    '2 Bedroom House': ('MARCH2024MEDIAN_HOUSE2', 'MARCH2024COUNT_HOUSE2'),\n",
    "    '3 Bedroom House': ('MARCH2024MEDIAN_HOUSE3', 'MARCH2024COUNT_HOUSE3'),\n",
    "    '4 Bedroom House': ('MARCH2024MEDIAN_HOUSE4', 'MARCH2024COUNT_HOUSE4')\n",
    "}\n",
    "\n",
    "# Plot for each property type\n",
    "for property_type, (median_col, count_col) in property_types.items():\n",
    "    print(f\"Plotting for {property_type}...\")\n",
    "    \n",
    "    # Plot top 5 and bottom 5 median rent prices\n",
    "    plot_top_bottom(merged_df, median_col, f\"Median Rent Prices ({property_type})\", \"Median Rent Price\")\n",
    "    \n",
    "    # Plot top 5 and bottom 5 count of properties\n",
    "    plot_top_bottom(merged_df, count_col, f\"Rent Counts ({property_type})\", \"Rent Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the bounding box for Victoria\n",
    "min_lat, min_lon, max_lat, max_lon = -39.2, 140.96, -33.9, 150.03\n",
    "\n",
    "# Construct the Overpass query to search for train stations operated by \"Metro Trains Melbourne\"\n",
    "overpass_query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "  node[\"railway\"=\"station\"][\"operator\"=\"Metro Trains Melbourne\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"railway\"=\"station\"][\"network\"=\"Public Transport Victoria\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"railway\"=\"station\"][\"operator\"~\"V/Line|V/line|VLine\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"public_transport\"=\"station\"][\"train\"=\"yes\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    ");\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the Overpass API\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: Received status code {response.status_code}\")\n",
    "    print(\"Response text:\", response.text)\n",
    "else:\n",
    "    # Attempt to decode the JSON response\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except ValueError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        print(\"Response text:\", response.text)\n",
    "    \n",
    "    # Extract train station coordinates and names\n",
    "    stations = []\n",
    "    for element in data['elements']:\n",
    "        if 'lat' in element and 'lon' in element:\n",
    "            stations.append({\n",
    "                'lat': element['lat'],\n",
    "                'lon': element['lon'],\n",
    "                'name': element['tags'].get('name', 'Unknown'),\n",
    "                'operator': element['tags'].get('operator', 'Unknown')\n",
    "            })\n",
    "    \n",
    "    # Convert the list of stations to a DataFrame\n",
    "    stations_df = pd.DataFrame(stations)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    stations_df.to_csv('../data/overpass_amenities/metro_train_stations_victoria.csv', index=False)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Train station data (Metro Trains Melbourne) saved to 'metro_train_stations_victoria.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Woolworths, Coles and Aldi and Other Fresh Food shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the bounding box for Victoria\n",
    "min_lat, min_lon, max_lat, max_lon = -39.2, 140.96, -33.9, 150.03\n",
    "\n",
    "# Construct the Overpass query to search for places selling fresh food (supermarkets, greengrocers)\n",
    "overpass_query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "  node[\"shop\"=\"supermarket\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"shop\"=\"greengrocer\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"shop\"=\"grocery\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"shop\"=\"convenience\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    ");\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the Overpass API\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: Received status code {response.status_code}\")\n",
    "    print(\"Response text:\", response.text)\n",
    "else:\n",
    "    # Attempt to decode the JSON response\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except ValueError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        print(\"Response text:\", response.text)\n",
    "    \n",
    "    # Extract supermarket and greengrocer coordinates and names\n",
    "    fresh_food_places = []\n",
    "    for element in data['elements']:\n",
    "        if 'lat' in element and 'lon' in element:\n",
    "            fresh_food_places.append({\n",
    "                'lat': element['lat'],\n",
    "                'lon': element['lon'],\n",
    "                'name': element['tags'].get('name', 'Unknown'),\n",
    "                'brand': element['tags'].get('brand', 'Unknown'),\n",
    "                'shop': element['tags'].get('shop', 'Unknown')\n",
    "            })\n",
    "    \n",
    "    # Convert the list of places to a DataFrame\n",
    "    fresh_food_df = pd.DataFrame(fresh_food_places)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    fresh_food_df.to_csv('../data/overpass_amenities/fresh_food_locations_victoria.csv', index=False)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Fresh food location data saved to 'fresh_food_locations_victoria.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the bounding box for Victoria\n",
    "min_lat, min_lon, max_lat, max_lon = -39.2, 140.96, -33.9, 150.03\n",
    "\n",
    "# Construct the Overpass query to search for public and private schools\n",
    "overpass_query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "  node[\"amenity\"=\"school\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"amenity\"=\"school\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  relation[\"amenity\"=\"school\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"building\"=\"school\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"building\"=\"school\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    ");\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the Overpass API\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: Received status code {response.status_code}\")\n",
    "    print(\"Response text:\", response.text)\n",
    "else:\n",
    "    # Attempt to decode the JSON response\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except ValueError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        print(\"Response text:\", response.text)\n",
    "    \n",
    "    # Extract school coordinates and details\n",
    "    schools = []\n",
    "    for element in data['elements']:\n",
    "        if 'lat' in element and 'lon' in element:\n",
    "            schools.append({\n",
    "                'lat': element['lat'],\n",
    "                'lon': element['lon'],\n",
    "                'name': element['tags'].get('name', 'Unknown'),\n",
    "                'operator': element['tags'].get('operator', 'Unknown'),\n",
    "                'amenity': element['tags'].get('amenity', 'Unknown')\n",
    "            })\n",
    "\n",
    "    # Convert the list of schools to a DataFrame\n",
    "    schools_df = pd.DataFrame(schools)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    schools_df.to_csv('../data/overpass_amenities/public_schools_victoria.csv', index=False)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"School location data saved to 'public_schools_victoria.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Childcare services "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the bounding box for Victoria\n",
    "min_lat, min_lon, max_lat, max_lon = -39.2, 140.96, -33.9, 150.03\n",
    "\n",
    "# Construct the Overpass query to search for childcare services (kindergarten and childcare centers)\n",
    "overpass_query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "  node[\"amenity\"~\"kindergarten|childcare\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"amenity\"~\"kindergarten|childcare\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"social_facility\"=\"day_care\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"social_facility\"=\"day_care\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    ");\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the Overpass API\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: Received status code {response.status_code}\")\n",
    "    print(\"Response text:\", response.text)\n",
    "else:\n",
    "    # Attempt to decode the JSON response\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except ValueError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        print(\"Response text:\", response.text)\n",
    "    \n",
    "    # Extract childcare service coordinates and details\n",
    "    childcare_services = []\n",
    "    for element in data['elements']:\n",
    "        if 'lat' in element and 'lon' in element:\n",
    "            childcare_services.append({\n",
    "                'lat': element['lat'],\n",
    "                'lon': element['lon'],\n",
    "                'name': element['tags'].get('name', 'Unknown'),\n",
    "                'amenity': element['tags'].get('amenity', 'Unknown')\n",
    "            })\n",
    "    \n",
    "    # Convert the list of childcare services to a DataFrame\n",
    "    childcare_df = pd.DataFrame(childcare_services)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    childcare_df.to_csv('../data/overpass_amenities/childcare_services_victoria.csv', index=False)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Childcare service data saved to 'childcare_services_victoria.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the bounding box for Victoria (latitude and longitude bounds)\n",
    "min_lat, min_lon, max_lat, max_lon = -39.2, 140.96, -33.9, 150.03\n",
    "\n",
    "# Construct the Overpass query to search for health services\n",
    "overpass_query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "  node[\"amenity\"~\"hospital|clinic|doctors|dentist|pharmacy\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"amenity\"~\"hospital|clinic|doctors|dentist|pharmacy\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"healthcare\"~\"hospital|clinic|doctor|dentist|pharmacy\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"healthcare\"~\"hospital|clinic|doctor|dentist|pharmacy\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    ");\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the Overpass API\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: Received status code {response.status_code}\")\n",
    "    print(\"Response text:\", response.text)\n",
    "else:\n",
    "    # Decode the JSON response\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except ValueError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        print(\"Response text:\", response.text)\n",
    "    \n",
    "    # Extract health service coordinates and details\n",
    "    health_services = []\n",
    "    for element in data['elements']:\n",
    "        if 'lat' in element and 'lon' in element:\n",
    "            health_services.append({\n",
    "                'lat': element['lat'],\n",
    "                'lon': element['lon'],\n",
    "                'name': element['tags'].get('name', 'Unknown'),\n",
    "                'amenity': element['tags'].get('amenity', 'Unknown')\n",
    "            })\n",
    "    \n",
    "    # Convert the list of health services to a DataFrame\n",
    "    health_services_df = pd.DataFrame(health_services)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    health_services_df.to_csv('../data/overpass_amenities/health_services_victoria.csv', index=False)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Health services data saved to 'health_services_victoria.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sports and Recreation Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the bounding box for Victoria (latitude and longitude bounds)\n",
    "min_lat, min_lon, max_lat, max_lon = -39.2, 140.96, -33.9, 150.03\n",
    "\n",
    "# Construct the Overpass query to search for sport and recreation facilities\n",
    "overpass_query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "  node[\"leisure\"=\"fitness_centre\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"leisure\"=\"fitness_centre\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"sport\"=\"fitness\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"sport\"=\"fitness\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"name\"~\"Fitness|Gym|Health Club|24/7\"](if:t[\"leisure\"]!=\"fitness_centre\")({min_lat},{min_lon},{max_lat},{max_lon});\n",
    ");\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the Overpass API\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: Received status code {response.status_code}\")\n",
    "    print(\"Response text:\", response.text)\n",
    "else:\n",
    "    # Decode the JSON response\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except ValueError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        print(\"Response text:\", response.text)\n",
    "    \n",
    "    # Extract sport and recreation facility coordinates and details\n",
    "    recreation_facilities = []\n",
    "    for element in data['elements']:\n",
    "        if 'lat' in element and 'lon' in element:\n",
    "            recreation_facilities.append({\n",
    "                'lat': element['lat'],\n",
    "                'lon': element['lon'],\n",
    "                'name': element['tags'].get('name', 'Unknown'),\n",
    "                'amenity': element['tags'].get('amenity', 'Unknown')\n",
    "            })\n",
    "    \n",
    "    # Convert the list of recreation facilities to a DataFrame\n",
    "    recreation_facilities_df = pd.DataFrame(recreation_facilities)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    recreation_facilities_df.to_csv('../data/overpass_amenities/recreation_facilities_victoria.csv', index=False)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Sport and recreation facilities data saved to 'recreation_facilities_victoria.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convienience Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define the bounding box for Victoria (latitude and longitude bounds)\n",
    "min_lat, min_lon, max_lat, max_lon = -39.2, 140.96, -33.9, 150.03\n",
    "\n",
    "# Construct the Overpass query to search for convenience stores\n",
    "overpass_query = f\"\"\"\n",
    "[out:json];\n",
    "(\n",
    "  node[\"shop\"~\"convenience|grocery|supermarket\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"shop\"~\"convenience|grocery|supermarket\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  node[\"amenity\"=\"marketplace\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    "  way[\"amenity\"=\"marketplace\"]({min_lat},{min_lon},{max_lat},{max_lon});\n",
    ");\n",
    "out center;\n",
    "\"\"\"\n",
    "\n",
    "# Send the request to the Overpass API\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error: Received status code {response.status_code}\")\n",
    "    print(\"Response text:\", response.text)\n",
    "else:\n",
    "    # Decode the JSON response\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except ValueError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        print(\"Response text:\", response.text)\n",
    "    \n",
    "    # Extract convenience store coordinates and details\n",
    "    convenience_stores = []\n",
    "    for element in data['elements']:\n",
    "        if 'lat' in element and 'lon' in element:\n",
    "            convenience_stores.append({\n",
    "                'lat': element['lat'],\n",
    "                'lon': element['lon'],\n",
    "                'name': element['tags'].get('name', 'Unknown'),\n",
    "                'shop': element['tags'].get('shop', 'Unknown')\n",
    "            })\n",
    "    \n",
    "    # Convert the list of convenience stores to a DataFrame\n",
    "    convenience_stores_df = pd.DataFrame(convenience_stores)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    convenience_stores_df.to_csv('../data/overpass_amenities/convenience_stores_victoria.csv', index=False)\n",
    "    \n",
    "    # Print success message\n",
    "    print(\"Convenience stores data saved to 'convenience_stores_victoria.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding to Suburbs Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the suburb shapefile\n",
    "suburbs_gdf = gpd.read_file(\"../data/mappings/GDA94/vic_localities.shp\")\n",
    "\n",
    "# Ensure the coordinate reference system (CRS) is correct (should be the same as your amenity data)\n",
    "suburbs_gdf = suburbs_gdf.to_crs(epsg=4326)\n",
    "\n",
    "suburbs_gdf['centroid'] = suburbs_gdf.geometry.centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train stations data\n",
    "train_stations_df = pd.read_csv('../data/overpass_amenities/metro_train_stations_victoria.csv')\n",
    "\n",
    "# Step 3: Convert the CSV data into a GeoDataFrame\n",
    "train_stations_gdf = gpd.GeoDataFrame(\n",
    "    train_stations_df,\n",
    "    geometry=[Point(xy) for xy in zip(train_stations_df.lon, train_stations_df.lat)],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Step 4: Calculate the centroids of each suburb\n",
    "suburbs_gdf['centroid'] = suburbs_gdf.geometry.centroid\n",
    "\n",
    "# Step 5: Create a buffer of 7.5 km around each centroid\n",
    "suburbs_gdf['buffer'] = suburbs_gdf['centroid'].buffer(7500)  # 7.5 km = 7500 meters\n",
    "\n",
    "# Step 6: Perform a spatial join to find train stations within each suburb's buffer\n",
    "train_stations_in_buffers = gpd.sjoin(train_stations_gdf, suburbs_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Step 7: Group by suburb and count the number of train stations in each buffer\n",
    "train_station_counts = train_stations_in_buffers.groupby('LOC_NAME').size().reset_index(name='train_station_count')\n",
    "\n",
    "# Step 8: Merge the counts back into the suburb GeoDataFrame\n",
    "suburbs_gdf = suburbs_gdf.merge(train_station_counts, how='left', left_on='LOC_NAME', right_on='LOC_NAME')\n",
    "\n",
    "# Fill any missing values (for suburbs with no train stations) with 0\n",
    "suburbs_gdf['train_station_count'] = suburbs_gdf['train_station_count'].fillna(0)\n",
    "\n",
    "# Optionally, drop the temporary columns used for calculations if desired\n",
    "suburbs_gdf.drop(columns=['centroid', 'buffer'], inplace=True)\n",
    "\n",
    "# Display the updated suburbs GeoDataFrame with train station counts\n",
    "print(suburbs_gdf[['LOC_NAME', 'train_station_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the CSV with fresh food locations\n",
    "fresh_food_df = pd.read_csv('../data/overpass_amenities/fresh_food_locations_victoria.csv')\n",
    "\n",
    "# Step 3: Convert the CSV data into a GeoDataFrame\n",
    "fresh_food_gdf = gpd.GeoDataFrame(\n",
    "    fresh_food_df,\n",
    "    geometry=[Point(xy) for xy in zip(fresh_food_df.lon, fresh_food_df.lat)],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Step 4: Calculate the centroids of each suburb (if not already done)\n",
    "suburbs_gdf['centroid'] = suburbs_gdf.geometry.centroid\n",
    "\n",
    "# Step 5: Create a buffer of 7.5 km around each centroid\n",
    "suburbs_gdf['buffer'] = suburbs_gdf['centroid'].buffer(7500)  # 7.5 km = 7500 meters\n",
    "\n",
    "# Step 6: Perform a spatial join to find fresh food locations within each suburb's buffer\n",
    "fresh_food_in_buffers = gpd.sjoin(fresh_food_gdf, suburbs_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Step 7: Group by suburb and count the number of fresh food locations in each buffer\n",
    "fresh_food_counts = fresh_food_in_buffers.groupby('LOC_NAME').size().reset_index(name='fresh_food_count')\n",
    "\n",
    "# Step 8: Merge the counts back into the suburb GeoDataFrame\n",
    "suburbs_gdf = suburbs_gdf.merge(fresh_food_counts, how='left', left_on='LOC_NAME', right_on='LOC_NAME')\n",
    "\n",
    "# Fill any missing values (for suburbs with no fresh food locations) with 0\n",
    "suburbs_gdf['fresh_food_count'] = suburbs_gdf['fresh_food_count'].fillna(0)\n",
    "\n",
    "# Optionally, drop the temporary columns used for calculations if desired\n",
    "suburbs_gdf.drop(columns=['centroid', 'buffer'], inplace=True)\n",
    "\n",
    "# Display the updated suburbs GeoDataFrame with fresh food counts\n",
    "print(suburbs_gdf[['LOC_NAME', 'fresh_food_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the CSV with childcare locations\n",
    "childcare_df = pd.read_csv('../data/overpass_amenities/childcare_services_victoria.csv')\n",
    "\n",
    "# Step 3: Convert the CSV data into a GeoDataFrame\n",
    "childcare_gdf = gpd.GeoDataFrame(\n",
    "    childcare_df,\n",
    "    geometry=[Point(xy) for xy in zip(childcare_df.lon, childcare_df.lat)],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Step 4: Calculate the centroids of each suburb (if not already done)\n",
    "suburbs_gdf['centroid'] = suburbs_gdf.geometry.centroid\n",
    "\n",
    "# Step 5: Create a buffer of 7.5 km around each centroid\n",
    "suburbs_gdf['buffer'] = suburbs_gdf['centroid'].buffer(7500)  # 7.5 km = 7500 meters\n",
    "\n",
    "# Step 6: Perform a spatial join to find childcare locations within each suburb's buffer\n",
    "childcare_in_buffers = gpd.sjoin(childcare_gdf, suburbs_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Step 7: Group by suburb and count the number of childcare locations in each buffer\n",
    "childcare_counts = childcare_in_buffers.groupby('LOC_NAME').size().reset_index(name='childcare_count')\n",
    "\n",
    "# Step 8: Merge the counts back into the suburb GeoDataFrame\n",
    "suburbs_gdf = suburbs_gdf.merge(childcare_counts, how='left', left_on='LOC_NAME', right_on='LOC_NAME')\n",
    "\n",
    "# Fill any missing values (for suburbs with no childcare locations) with 0\n",
    "suburbs_gdf['childcare_count'] = suburbs_gdf['childcare_count'].fillna(0)\n",
    "\n",
    "# Optionally, drop the temporary columns used for calculations if desired\n",
    "suburbs_gdf.drop(columns=['centroid', 'buffer'], inplace=True)\n",
    "\n",
    "# Display the updated suburbs GeoDataFrame with childcare counts\n",
    "print(suburbs_gdf[['LOC_NAME', 'childcare_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the CSV with health services locations\n",
    "health_df = pd.read_csv('../data/overpass_amenities/health_services_victoria.csv')\n",
    "\n",
    "# Step 3: Convert the CSV data into a GeoDataFrame\n",
    "health_gdf = gpd.GeoDataFrame(\n",
    "    health_df,\n",
    "    geometry=[Point(xy) for xy in zip(health_df.lon, health_df.lat)],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Step 4: Calculate the centroids of each suburb (if not already done)\n",
    "suburbs_gdf['centroid'] = suburbs_gdf.geometry.centroid\n",
    "\n",
    "# Step 5: Create a buffer of 7.5 km around each centroid\n",
    "suburbs_gdf['buffer'] = suburbs_gdf['centroid'].buffer(7500)  # 7.5 km = 7500 meters\n",
    "\n",
    "# Step 6: Perform a spatial join to find health services within each suburb's buffer\n",
    "health_in_buffers = gpd.sjoin(health_gdf, suburbs_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Step 7: Group by suburb and count the number of health services in each buffer\n",
    "health_counts = health_in_buffers.groupby('LOC_NAME').size().reset_index(name='health_count')\n",
    "\n",
    "# Step 8: Merge the counts back into the suburb GeoDataFrame\n",
    "suburbs_gdf = suburbs_gdf.merge(health_counts, how='left', left_on='LOC_NAME', right_on='LOC_NAME')\n",
    "\n",
    "# Fill any missing values (for suburbs with no health services) with 0\n",
    "suburbs_gdf['health_count'] = suburbs_gdf['health_count'].fillna(0)\n",
    "\n",
    "# Optionally, drop the temporary columns used for calculations if desired\n",
    "suburbs_gdf.drop(columns=['centroid', 'buffer'], inplace=True)\n",
    "\n",
    "# Display the updated suburbs GeoDataFrame with health counts\n",
    "print(suburbs_gdf[['LOC_NAME', 'health_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the CSV with recreation facilities locations\n",
    "recreation_df = pd.read_csv('../data/overpass_amenities/recreation_facilities_victoria.csv')\n",
    "\n",
    "# Step 3: Convert the CSV data into a GeoDataFrame\n",
    "recreation_gdf = gpd.GeoDataFrame(\n",
    "    recreation_df,\n",
    "    geometry=[Point(xy) for xy in zip(recreation_df.lon, recreation_df.lat)],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Step 4: Calculate the centroids of each suburb (if not already done)\n",
    "suburbs_gdf['centroid'] = suburbs_gdf.geometry.centroid\n",
    "\n",
    "# Step 5: Create a buffer of 7.5 km around each centroid\n",
    "suburbs_gdf['buffer'] = suburbs_gdf['centroid'].buffer(7500)  # 7.5 km = 7500 meters\n",
    "\n",
    "# Step 6: Perform a spatial join to find recreation facilities within each suburb's buffer\n",
    "recreation_in_buffers = gpd.sjoin(recreation_gdf, suburbs_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Step 7: Group by suburb and count the number of recreation facilities in each buffer\n",
    "recreation_counts = recreation_in_buffers.groupby('LOC_NAME').size().reset_index(name='recreation_count')\n",
    "\n",
    "# Step 8: Merge the counts back into the suburb GeoDataFrame\n",
    "suburbs_gdf = suburbs_gdf.merge(recreation_counts, how='left', left_on='LOC_NAME', right_on='LOC_NAME')\n",
    "\n",
    "# Fill any missing values (for suburbs with no recreation facilities) with 0\n",
    "suburbs_gdf['recreation_count'] = suburbs_gdf['recreation_count'].fillna(0)\n",
    "\n",
    "# Optionally, drop the temporary columns used for calculations if desired\n",
    "suburbs_gdf.drop(columns=['centroid', 'buffer'], inplace=True)\n",
    "\n",
    "# Display the updated suburbs GeoDataFrame with recreation counts\n",
    "print(suburbs_gdf[['LOC_NAME', 'recreation_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the CSV with convenience store locations\n",
    "convenience_stores_df = pd.read_csv('../data/overpass_amenities/convenience_stores_victoria.csv')\n",
    "\n",
    "# Step 3: Convert the CSV data into a GeoDataFrame\n",
    "convenience_stores_gdf = gpd.GeoDataFrame(\n",
    "    convenience_stores_df,\n",
    "    geometry=[Point(xy) for xy in zip(convenience_stores_df.lon, convenience_stores_df.lat)],\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Step 4: Calculate the centroids of each suburb (if not already done)\n",
    "suburbs_gdf['centroid'] = suburbs_gdf.geometry.centroid\n",
    "\n",
    "# Step 5: Create a buffer of 7.5 km around each centroid\n",
    "suburbs_gdf['buffer'] = suburbs_gdf['centroid'].buffer(7500)  # 7.5 km = 7500 meters\n",
    "\n",
    "# Step 6: Perform a spatial join to find convenience stores within each suburb's buffer\n",
    "convenience_stores_in_buffers = gpd.sjoin(convenience_stores_gdf, suburbs_gdf, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Step 7: Group by suburb and count the number of convenience stores in each buffer\n",
    "convenience_store_counts = convenience_stores_in_buffers.groupby('LOC_NAME').size().reset_index(name='convenience_store_count')\n",
    "\n",
    "# Step 8: Merge the counts back into the suburb GeoDataFrame\n",
    "suburbs_gdf = suburbs_gdf.merge(convenience_store_counts, how='left', left_on='LOC_NAME', right_on='LOC_NAME')\n",
    "\n",
    "# Fill any missing values (for suburbs with no convenience stores) with 0\n",
    "suburbs_gdf['convenience_store_count'] = suburbs_gdf['convenience_store_count'].fillna(0)\n",
    "\n",
    "# Optionally, drop the temporary columns used for calculations if desired\n",
    "suburbs_gdf.drop(columns=['centroid', 'buffer'], inplace=True)\n",
    "\n",
    "# Display the updated suburbs GeoDataFrame with convenience store counts\n",
    "print(suburbs_gdf[['LOC_NAME', 'convenience_store_count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Load the new dataset\n",
    "education_data = pd.read_csv('../data/overpass_amenities/schools_preprocessed.csv')  # Update the path to your dataset\n",
    "\n",
    "# Step 1: Aggregate the school counts into a single column\n",
    "education_data['number_of_schools'] = (education_data['num_primary'] +\n",
    "                                        education_data['num_secondary_public'] +\n",
    "                                        education_data['num_secondary_private'] +\n",
    "                                        education_data['num_secondary_catholic'])\n",
    "\n",
    "# Step 2: Remove unnecessary columns from the education dataset\n",
    "education_data = education_data[['suburb', 'number_of_schools']]  # Keep only relevant columns\n",
    "\n",
    "# Prepare suburb names for matching\n",
    "education_data['suburb'] = education_data['suburb'].str.lower()  # Convert to lowercase\n",
    "\n",
    "# Ensure suburbs_gdf is a GeoDataFrame and prepare for matching\n",
    "if not isinstance(suburbs_gdf, gpd.GeoDataFrame):\n",
    "    suburbs_gdf = gpd.GeoDataFrame(suburbs_gdf, geometry='geometry')\n",
    "\n",
    "suburbs_gdf['LOC_NAME'] = suburbs_gdf['LOC_NAME'].str.lower()  # Convert to lowercase\n",
    "\n",
    "# Step 3: Fuzzy match suburb names and create a mapping\n",
    "def fuzzy_match(row, choices, scorer):\n",
    "    match, score = process.extractOne(row['suburb'], choices, scorer=scorer)\n",
    "    return match if score >= 80 else None  # Adjust threshold as necessary\n",
    "\n",
    "# Create a mapping for suburb matching\n",
    "suburb_choices = suburbs_gdf['LOC_NAME'].unique()\n",
    "education_data['matched_suburb'] = education_data.apply(\n",
    "    fuzzy_match, \n",
    "    choices=suburb_choices, \n",
    "    scorer=process.fuzz.token_sort_ratio,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 4: Perform a left join on suburbs_gdf with the matched suburbs\n",
    "suburbs_gdf = suburbs_gdf.merge(\n",
    "    education_data,\n",
    "    left_on='LOC_NAME', \n",
    "    right_on='matched_suburb',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Step 5: Fill NaN values in the number_of_schools column with 0 for suburbs with no match\n",
    "suburbs_gdf['number_of_schools'] = suburbs_gdf['number_of_schools'].fillna(0)\n",
    "\n",
    "# Convert the number_of_schools to integer if necessary\n",
    "suburbs_gdf['number_of_schools'] = suburbs_gdf['number_of_schools'].astype(int)\n",
    "\n",
    "# Step 6: Remove unnecessary columns\n",
    "suburbs_gdf = suburbs_gdf.drop(columns=[\n",
    "    'suburb',  # Original suburb name\n",
    "    'matched_suburb'  # Column used for matching\n",
    "], errors='ignore')  # Use errors='ignore' to avoid issues if columns don't exist\n",
    "\n",
    "# Now, suburbs_gdf will have the additional column 'number_of_schools' and be cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburbs_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Step 2: Prepare for fuzzy matching\n",
    "# Ensure both columns are in a comparable format\n",
    "merged_df['Suburbs'] = merged_df['Suburbs'].str.strip().str.title()\n",
    "suburbs_gdf['LOC_NAME'] = suburbs_gdf['LOC_NAME'].str.strip().str.title()\n",
    "\n",
    "# Step 3: Create a function to find the best match\n",
    "def get_best_match(suburb, choices):\n",
    "    match, score = process.extractOne(suburb, choices)\n",
    "    return match, score\n",
    "\n",
    "# Step 4: Apply fuzzy matching\n",
    "merged_df['Best_Match'], merged_df['Match_Score'] = zip(*merged_df['Suburbs'].apply(lambda x: get_best_match(x, suburbs_gdf['LOC_NAME'].tolist())))\n",
    "\n",
    "# Step 5: Set a threshold for matching (e.g., 80% confidence)\n",
    "threshold = 80\n",
    "matched_suburbs = merged_df[merged_df['Match_Score'] >= threshold]\n",
    "\n",
    "# Step 6: Merge with the suburbs GeoDataFrame using the best matches\n",
    "suburbs_gdf_merged = suburbs_gdf.merge(\n",
    "    matched_suburbs,\n",
    "    how='inner',  # Only keep rows where there's a match\n",
    "    left_on='LOC_NAME',  # Suburb identifier in the GeoDataFrame\n",
    "    right_on='Best_Match'  # Best matched suburb identifier in the rental data\n",
    ")\n",
    "\n",
    "# Optional: Drop unnecessary columns\n",
    "suburbs_gdf_merged = suburbs_gdf_merged.drop(columns=['Suburbs', 'Best_Match', 'Match_Score'])\n",
    "\n",
    "# Step 7: Inspect the merged DataFrame\n",
    "print(suburbs_gdf_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove unwanted columns from both sides of the merge\n",
    "columns_to_delete = [\n",
    "    'LC_PLY_PID_x', 'LOC_PID_x', 'DT_CREATE_x', 'LOC_CLASS_x', 'STATE_x', \n",
    "    'LC_PLY_PID_y', 'LOC_PID_y', 'DT_CREATE_y', 'LOC_CLASS_y', 'STATE_y', \n",
    "    'geometry_y'  # Assuming you want to keep geometry from the suburbs\n",
    "]\n",
    "\n",
    "# Drop unwanted columns\n",
    "suburbs_gdf_merged = suburbs_gdf_merged.drop(columns=columns_to_delete)\n",
    "\n",
    "# Step 2: Rename 'LOC_NAME_x' to 'Suburb' (keeping the 'LOC_NAME_x' because of merge)\n",
    "suburbs_gdf_merged = suburbs_gdf_merged.rename(columns={'LOC_NAME_x': 'Suburb'})\n",
    "\n",
    "# Step 3: Rename the rental columns\n",
    "column_renames = {\n",
    "    'MARCH2024COUNT_FLAT1': '1 Bedroom Flat Count',\n",
    "    'MARCH2024MEDIAN_FLAT1': '1 Bedroom Flat Median',\n",
    "    'MARCH2024COUNT_FLAT2': '2 Bedroom Flat Count',\n",
    "    'MARCH2024MEDIAN_FLAT2': '2 Bedroom Flat Median',\n",
    "    'MARCH2024COUNT_FLAT3': '3 Bedroom Flat Count',\n",
    "    'MARCH2024MEDIAN_FLAT3': '3 Bedroom Flat Median',\n",
    "    'MARCH2024COUNT_HOUSE2': '2 Bedroom House Count',\n",
    "    'MARCH2024MEDIAN_HOUSE2': '2 Bedroom House Median',\n",
    "    'MARCH2024COUNT_HOUSE3': '3 Bedroom House Count',\n",
    "    'MARCH2024MEDIAN_HOUSE3': '3 Bedroom House Median',\n",
    "    'MARCH2024COUNT_HOUSE4': '4 Bedroom House Count',\n",
    "    'MARCH2024MEDIAN_HOUSE4': '4 Bedroom House Median'\n",
    "}\n",
    "\n",
    "# Rename rental columns (if they are still present)\n",
    "suburbs_gdf_merged = suburbs_gdf_merged.rename(columns=column_renames)\n",
    "\n",
    "# Step 4: Select relevant columns to keep, including amenity counts\n",
    "# Ensure you include amenity counts in the final DataFrame\n",
    "amenity_columns = [\n",
    "    'train_station_count', \n",
    "    'fresh_food_count',\n",
    "    'childcare_count', \n",
    "    'health_count', \n",
    "    'recreation_count',\n",
    "    'convenience_store_count',\n",
    "    'number_of_schools'\n",
    "]\n",
    "\n",
    "# Combine all relevant columns to keep in the final DataFrame\n",
    "final_columns = ['Suburb', 'geometry_x'] + list(column_renames.values()) + amenity_columns\n",
    "\n",
    "# Select the final columns to keep\n",
    "suburbs_gdf_merged = suburbs_gdf_merged[final_columns]\n",
    "\n",
    "# Optionally, rename the geometry column to 'geometry'\n",
    "suburbs_gdf_merged = suburbs_gdf_merged.rename(columns={'geometry_x': 'geometry'})\n",
    "\n",
    "# Step 5: Inspect the cleaned DataFrame\n",
    "print(suburbs_gdf_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburbs_gdf_merged.head()\n",
    "print(suburbs_gdf_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Normalize the relevant columns including median values\n",
    "columns_to_normalize = [\n",
    "    'train_station_count', 'fresh_food_count', 'childcare_count', 'health_count',\n",
    "    'recreation_count', 'convenience_store_count', 'number_of_schools',\n",
    "    '1 Bedroom Flat Count', '1 Bedroom Flat Median',\n",
    "    '2 Bedroom Flat Count', '2 Bedroom Flat Median',\n",
    "    '3 Bedroom Flat Count', '3 Bedroom Flat Median',\n",
    "    '2 Bedroom House Count', '2 Bedroom House Median',\n",
    "    '3 Bedroom House Count', '3 Bedroom House Median',\n",
    "    '4 Bedroom House Count', '4 Bedroom House Median'\n",
    "]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_values = scaler.fit_transform(suburbs_gdf_merged[columns_to_normalize])\n",
    "\n",
    "# Create a DataFrame with the normalized values\n",
    "normalized_df = pd.DataFrame(normalized_values, columns=columns_to_normalize)\n",
    "\n",
    "weighting_schemes = {\n",
    "    'families_with_kids': {\n",
    "        'train_station_count': 0.03,  # Lower emphasis on count\n",
    "        'fresh_food_count': 0.08,\n",
    "        'childcare_count': 0.20,\n",
    "        'health_count': 0.05,\n",
    "        'recreation_count': 0.10,\n",
    "        'convenience_store_count': 0.05,\n",
    "        'number_of_schools': 0.4,\n",
    "        '1 Bedroom Flat Count': 0.02,\n",
    "        '1 Bedroom Flat Median': 0.15,  # Increased weight on median\n",
    "        '2 Bedroom Flat Count': 0.02,\n",
    "        '2 Bedroom Flat Median': 0.20,  # Higher weight for larger flats\n",
    "        '3 Bedroom Flat Count': 0.02,\n",
    "        '3 Bedroom Flat Median': 0.25,  # Higher weight for larger flats\n",
    "        '2 Bedroom House Count': 0.03,\n",
    "        '2 Bedroom House Median': 0.25,  # Increased weight on median rents\n",
    "        '3 Bedroom House Count': 0.03,\n",
    "        '3 Bedroom House Median': 0.30,  # Higher weight for larger houses\n",
    "        '4 Bedroom House Count': 0.02,\n",
    "        '4 Bedroom House Median': 0.35,  # Emphasizing larger properties\n",
    "    },\n",
    "    'working_professionals': {\n",
    "        'train_station_count': 0.15,  # Higher emphasis on transportation\n",
    "        'fresh_food_count': 0.08,\n",
    "        'childcare_count': 0.01,\n",
    "        'health_count': 0.10,\n",
    "        'recreation_count': 0.10,\n",
    "        'convenience_store_count': 0.10,\n",
    "        'number_of_schools': 0.2,\n",
    "        '1 Bedroom Flat Count': 0.05,\n",
    "        '1 Bedroom Flat Median': 0.20,  # Increased weight on median\n",
    "        '2 Bedroom Flat Count': 0.02,\n",
    "        '2 Bedroom Flat Median': 0.15,\n",
    "        '3 Bedroom Flat Count': 0.02,\n",
    "        '3 Bedroom Flat Median': 0.20,\n",
    "        '2 Bedroom House Count': 0.02,\n",
    "        '2 Bedroom House Median': 0.15,\n",
    "        '3 Bedroom House Count': 0.02,\n",
    "        '3 Bedroom House Median': 0.20,\n",
    "        '4 Bedroom House Count': 0.02,\n",
    "        '4 Bedroom House Median': 0.20,\n",
    "    },\n",
    "    'elderly': {\n",
    "        'train_station_count': 0.02,\n",
    "        'fresh_food_count': 0.15,\n",
    "        'childcare_count': 0.01,\n",
    "        'health_count': 0.35,  # Higher weight for health services\n",
    "        'recreation_count': 0.10,\n",
    "        'convenience_store_count': 0.20,\n",
    "        'number_of_schools': 0.05,\n",
    "        '1 Bedroom Flat Count': 0.03,\n",
    "        '1 Bedroom Flat Median': 0.15,\n",
    "        '2 Bedroom Flat Count': 0.02,\n",
    "        '2 Bedroom Flat Median': 0.15,\n",
    "        '3 Bedroom Flat Count': 0.02,\n",
    "        '3 Bedroom Flat Median': 0.15,\n",
    "        '2 Bedroom House Count': 0.02,\n",
    "        '2 Bedroom House Median': 0.20,\n",
    "        '3 Bedroom House Count': 0.02,\n",
    "        '3 Bedroom House Median': 0.25,\n",
    "        '4 Bedroom House Count': 0.02,\n",
    "        '4 Bedroom House Median': 0.1,  # Emphasizing larger properties\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to calculate livability score based on demographic\n",
    "def calculate_livability_score(df, demographic):\n",
    "    weights = weighting_schemes[demographic]\n",
    "    \n",
    "    # Normalize the relevant columns\n",
    "    normalized_values = scaler.fit_transform(df[columns_to_normalize])\n",
    "    normalized_df = pd.DataFrame(normalized_values, columns=columns_to_normalize)\n",
    "\n",
    "    # Calculate the livability score\n",
    "    df['livability_score'] = (normalized_df * pd.Series(weights)).sum(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "suburbs_with_family_scores = calculate_livability_score(suburbs_gdf_merged, 'families_with_kids')\n",
    "suburbs_with_professional_scores = calculate_livability_score(suburbs_gdf_merged, 'working_professionals')\n",
    "suburbs_with_elderly_scores = calculate_livability_score(suburbs_gdf_merged, 'elderly')\n",
    "\n",
    "# Optional: Sort by livability score\n",
    "suburbs_family_sorted = suburbs_with_family_scores.sort_values(by='livability_score', ascending=False)\n",
    "suburbs_professional_sorted = suburbs_with_professional_scores.sort_values(by='livability_score', ascending=False)\n",
    "suburbs_elderly_sorted = suburbs_with_elderly_scores.sort_values(by='livability_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming suburbs_family_sorted, suburbs_professional_sorted, and suburbs_elderly_sorted are your DataFrames\n",
    "\n",
    "# Get the top 10 ranked suburbs for families with kids\n",
    "top_families = suburbs_family_sorted.nlargest(10, 'livability_score')\n",
    "\n",
    "# Get the top 10 ranked suburbs for working professionals\n",
    "top_professionals = suburbs_professional_sorted.nlargest(10, 'livability_score')\n",
    "\n",
    "# Get the top 10 ranked suburbs for the elderly\n",
    "top_elderly = suburbs_elderly_sorted.nlargest(10, 'livability_score')\n",
    "\n",
    "# Display the top 10 for Families with Kids\n",
    "print(\"Top 10 Livability Scores for Families with Kids:\")\n",
    "print(tabulate(top_families[['Suburb', 'livability_score']], headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "# Display the top 10 for Working Professionals\n",
    "print(\"\\nTop 10 Livability Scores for Working Professionals:\")\n",
    "print(tabulate(top_professionals[['Suburb', 'livability_score']], headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "# Display the top 10 for the Elderly\n",
    "print(\"\\nTop 10 Livability Scores for the Elderly:\")\n",
    "print(tabulate(top_elderly[['Suburb', 'livability_score']], headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from tabulate import tabulate\n",
    "import pickle\n",
    "\n",
    "# Assuming suburbs_family_sorted is your DataFrame containing the suburbs\n",
    "\n",
    "# Get the top 11 ranked suburbs for families with kids\n",
    "top_families = suburbs_family_sorted.nlargest(11, 'livability_score')\n",
    "\n",
    "# Display the top 11 for Families with Kids\n",
    "print(\"Top 11 Livability Scores for Families with Kids:\")\n",
    "print(tabulate(top_families[['Suburb', 'livability_score']], headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "# Create a Folium map centered on Melbourne\n",
    "m = folium.Map(location=[-37.8136, 144.9631], zoom_start=10)  # Centered on Melbourne\n",
    "\n",
    "# Define a single color for the markers\n",
    "marker_color = 'blue'  # Change this to your preferred color\n",
    "\n",
    "# Add markers for suburbs ranked 2 to 11\n",
    "for idx in range(1, len(top_families)):  # Start from index 1 to skip the top suburb\n",
    "    row = top_families.iloc[idx]\n",
    "    suburb_name = row['Suburb']\n",
    "    suburb_score = row['livability_score']\n",
    "\n",
    "    # Get the geometry for the suburb (assuming it exists in a GeoDataFrame)\n",
    "    suburb_geom = suburbs_gdf[suburbs_gdf['LOC_NAME'] == suburb_name].geometry.iloc[0]\n",
    "\n",
    "    # Calculate the centroid for the suburb geometry\n",
    "    centroid = suburb_geom.centroid\n",
    "\n",
    "    # Ranking for display (1 for 2nd suburb, 10 for 11th suburb)\n",
    "    rank = idx  # Since idx starts from 1, it corresponds directly to the ranking from 1 to 10\n",
    "\n",
    "    # Create a marker for the suburb with its rank\n",
    "    folium.Marker(\n",
    "        location=[centroid.y, centroid.x],\n",
    "        popup=f\"Rank {rank}: {suburb_name} - {suburb_score:.2f} livability score\",\n",
    "        icon=folium.Icon(color=marker_color, icon='info-sign')\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Highlight the suburb on the map (with a transparent fill)\n",
    "    folium.GeoJson(\n",
    "        suburb_geom,\n",
    "        style_function=lambda x: {\n",
    "            'fillColor': 'lightgray',\n",
    "            'color': 'black',\n",
    "            'weight': 2,\n",
    "            'fillOpacity': 0.4,  # Semi-transparent highlight\n",
    "        },\n",
    "    ).add_to(m)\n",
    "\n",
    "display(m)\n",
    "m.save('../data/curated/liveability/top10_livemap.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(suburbs_gdf_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import LinearColormap\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Define the suburb names\n",
    "suburbs_list = [\n",
    "    'Box Hill', 'Melbourne CBD - North', 'Doncaster', 'Glen Waverley - East', \n",
    "    'Glen Waverley - West', 'Balwyn North', 'Box Hill North', 'Balwyn', \n",
    "    'Wheelers Hill', 'Doncaster East - South', 'Mount Waverley - North', \n",
    "    'Mount Waverley - South', 'Melbourne CBD - West', 'Doncaster East - North', \n",
    "    'Carlton', 'Wantirna South', 'Blackburn', 'Vermont South', \n",
    "    'Templestowe', 'Docklands', 'Glen Waverley'\n",
    "]\n",
    "\n",
    "# Ensure suburbs_gdf_merged is a GeoDataFrame\n",
    "if not isinstance(suburbs_gdf_merged, gpd.GeoDataFrame):\n",
    "    suburbs_gdf_merged = gpd.GeoDataFrame(suburbs_gdf_merged, geometry='geometry')\n",
    "\n",
    "# Set the CRS (use the appropriate CRS for your data)\n",
    "suburbs_gdf_merged = suburbs_gdf_merged.set_crs(\"EPSG:4326\")\n",
    "\n",
    "# Filter the GeoDataFrame to include only the listed suburbs\n",
    "result_suburbs = suburbs_gdf_merged[suburbs_gdf_merged['Suburb'].isin(suburbs_list)]\n",
    "\n",
    "# Sort by livability score for better clarity\n",
    "result_suburbs = result_suburbs.sort_values('livability_score', ascending=False)\n",
    "\n",
    "# Create a Folium map centered on Melbourne\n",
    "m = folium.Map(location=[-37.8136, 144.9631], zoom_start=10)  # Centered on Melbourne\n",
    "\n",
    "# Create a color map based on livability scores\n",
    "color_map = LinearColormap(\n",
    "    colors=['#f9d0c8', '#bd6877', '#4c4557'], \n",
    "    vmin=result_suburbs['livability_score'].min(), \n",
    "    vmax=result_suburbs['livability_score'].max()\n",
    ")\n",
    "\n",
    "# Add polygons to the map with livability score popups\n",
    "for idx, row in result_suburbs.iterrows():\n",
    "    # Add GeoJson for the suburb with a specific color based on livability score\n",
    "    folium.GeoJson(\n",
    "        row['geometry'],\n",
    "        style_function=lambda x, score=row['livability_score']: {\n",
    "            'fillColor': color_map(score),\n",
    "            'color': 'black',\n",
    "            'weight': 2,\n",
    "            'fillOpacity': 0.6,  # Adjust opacity\n",
    "        },\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add a marker for each suburb at its centroid with livability score in popup\n",
    "    centroid = row['geometry'].centroid\n",
    "    folium.Marker(\n",
    "        location=[centroid.y, centroid.x],\n",
    "        popup=f\"{row['Suburb']}: {row['livability_score']:.2f} livability score\",\n",
    "        icon=folium.Icon(color='blue')\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add color legend to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Prepare the DataFrame for display\n",
    "styled_table = result_suburbs[['Suburb', 'livability_score']]\n",
    "\n",
    "# Format the DataFrame for better visibility\n",
    "styled_table['livability_score'] = styled_table['livability_score'].map('{:.2f}'.format)\n",
    "\n",
    "# Create an HTML table string\n",
    "html_table = styled_table.to_html(index=False, \n",
    "                                   classes='table table-striped', \n",
    "                                   border=0, \n",
    "                                   justify='center', \n",
    "                                   col_space=100)\n",
    "\n",
    "# Add custom CSS for styling\n",
    "styled_html = f'''\n",
    "<style>\n",
    ".table {{\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "}}\n",
    ".table th, .table td {{\n",
    "    border: 1px solid black;\n",
    "    padding: 8px;\n",
    "    text-align: center;\n",
    "}}\n",
    ".table th {{\n",
    "    background-color: #0f0f0f;\n",
    "    font-weight: bold;\n",
    "    text-align: left;\n",
    "}}\n",
    "</style>\n",
    "{html_table}\n",
    "'''\n",
    "\n",
    "# Display the styled table\n",
    "display(HTML(styled_html))\n",
    "\n",
    "# Display the map\n",
    "display(m)  # This will render the map in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure suburbs_gdf_merged is a GeoDataFrame\n",
    "if not isinstance(suburbs_gdf_merged, gpd.GeoDataFrame):\n",
    "    suburbs_gdf_merged = gpd.GeoDataFrame(suburbs_gdf_merged, geometry='geometry')\n",
    "\n",
    "# Sort the suburbs by livability_score in descending order\n",
    "suburbs_gdf_sorted = suburbs_gdf_merged[['Suburb', 'livability_score']].sort_values(by='livability_score', ascending=False)\n",
    "\n",
    "# Remove the first row\n",
    "suburbs_gdf_sorted = suburbs_gdf_sorted.iloc[1:]\n",
    "\n",
    "# Add a rank column to assign ranks based on the sorted livability scores\n",
    "suburbs_gdf_sorted['Rank'] = range(1, len(suburbs_gdf_sorted) + 1)\n",
    "\n",
    "# Save the result to a CSV file, with ranks included\n",
    "csv_file_path = \"../data/curated/liveability/output_suburbs_livability_ranked.csv\"  # Set the path where you want to save the CSV\n",
    "suburbs_gdf_sorted.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"CSV file has been saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Rental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(suburbs_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Normalize the relevant columns including median values\n",
    "columns_to_normalize = [\n",
    "    'train_station_count', 'fresh_food_count', 'childcare_count', 'health_count',\n",
    "    'recreation_count', 'convenience_store_count'\n",
    "]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "normalized_values = scaler.fit_transform(suburbs_gdf[columns_to_normalize])\n",
    "\n",
    "# Create a DataFrame with the normalized values\n",
    "normalized_df = pd.DataFrame(normalized_values, columns=columns_to_normalize)\n",
    "\n",
    "weighting_schemes = {\n",
    "    'families_with_kids': {\n",
    "        'train_station_count': 0.03,  # Lower emphasis on count\n",
    "        'fresh_food_count': 0.08,\n",
    "        'childcare_count': 0.20,\n",
    "        'health_count': 0.05,\n",
    "        'recreation_count': 0.10,\n",
    "        'convenience_store_count': 0.05,\n",
    "        'number_of_schools': 0.4,\n",
    "    },\n",
    "    'working_professionals': {\n",
    "        'train_station_count': 0.15,  # Higher emphasis on transportation\n",
    "        'fresh_food_count': 0.08,\n",
    "        'childcare_count': 0.01,\n",
    "        'health_count': 0.10,\n",
    "        'recreation_count': 0.10,\n",
    "        'convenience_store_count': 0.10,\n",
    "        'number_of_schools':0.2,\n",
    "    },\n",
    "    'elderly': {\n",
    "        'train_station_count': 0.02,\n",
    "        'fresh_food_count': 0.15,\n",
    "        'childcare_count': 0.01,\n",
    "        'health_count': 0.35,  # Higher weight for health services\n",
    "        'recreation_count': 0.10,\n",
    "        'convenience_store_count': 0.20,\n",
    "        'number_of_schools': 0.05,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to calculate livability score based on demographic\n",
    "def calculate_livability_score(df, demographic):\n",
    "    weights = weighting_schemes[demographic]\n",
    "    \n",
    "    # Normalize the relevant columns\n",
    "    normalized_values = scaler.fit_transform(df[columns_to_normalize])\n",
    "    normalized_df = pd.DataFrame(normalized_values, columns=columns_to_normalize)\n",
    "\n",
    "    # Calculate the livability score\n",
    "    df['livability_score'] = (normalized_df * pd.Series(weights)).sum(axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "suburbs_with_family_scores = calculate_livability_score(suburbs_gdf, 'families_with_kids')\n",
    "suburbs_with_professional_scores = calculate_livability_score(suburbs_gdf, 'working_professionals')\n",
    "suburbs_with_elderly_scores = calculate_livability_score(suburbs_gdf, 'elderly')\n",
    "\n",
    "# Optional: Sort by livability score\n",
    "suburbs_family_sorted = suburbs_with_family_scores.sort_values(by='livability_score', ascending=False)\n",
    "suburbs_professional_sorted = suburbs_with_professional_scores.sort_values(by='livability_score', ascending=False)\n",
    "suburbs_elderly_sorted = suburbs_with_elderly_scores.sort_values(by='livability_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming suburbs_family_sorted, suburbs_professional_sorted, and suburbs_elderly_sorted are your DataFrames\n",
    "\n",
    "# Get the top 10 ranked suburbs for families with kids\n",
    "top_families = suburbs_family_sorted.nlargest(10, 'livability_score')\n",
    "\n",
    "# Get the top 10 ranked suburbs for working professionals\n",
    "top_professionals = suburbs_professional_sorted.nlargest(10, 'livability_score')\n",
    "\n",
    "# Get the top 10 ranked suburbs for the elderly\n",
    "top_elderly = suburbs_elderly_sorted.nlargest(10, 'livability_score')\n",
    "\n",
    "# Display the top 10 for Families with Kids\n",
    "print(\"Top 10 Livability Scores for Families with Kids:\")\n",
    "print(tabulate(top_families[['LOC_NAME', 'livability_score']], headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "# Display the top 10 for Working Professionals\n",
    "print(\"\\nTop 10 Livability Scores for Working Professionals:\")\n",
    "print(tabulate(top_professionals[['LOC_NAME', 'livability_score']], headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "# Display the top 10 for the Elderly\n",
    "print(\"\\nTop 10 Livability Scores for the Elderly:\")\n",
    "print(tabulate(top_elderly[['LOC_NAME', 'livability_score']], headers='keys', tablefmt='psql', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(suburbs_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.ops import nearest_points\n",
    "from shapely.geometry import Point\n",
    "import folium\n",
    "from folium import LinearColormap\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def find_closest_suburb(point, gdf):\n",
    "    return gdf.geometry.distance(point).idxmin()\n",
    "\n",
    "# Ensure suburbs_gdf is a GeoDataFrame\n",
    "if not isinstance(suburbs_gdf, gpd.GeoDataFrame):\n",
    "    suburbs_gdf = gpd.GeoDataFrame(suburbs_gdf, geometry='geometry')\n",
    "\n",
    "# Ensure the geometry column is correctly set\n",
    "if 'geometry' not in suburbs_gdf.columns:\n",
    "    raise KeyError(\"The 'geometry' column is missing from suburbs_gdf.\")\n",
    "\n",
    "# Set the CRS (use the appropriate CRS for your data)\n",
    "suburbs_gdf = suburbs_gdf.set_crs(\"EPSG:4326\")\n",
    "\n",
    "# Rank all suburbs based on livability_score\n",
    "suburbs_gdf['Overall_Rank'] = suburbs_gdf['livability_score'].rank(ascending=False, method='min')\n",
    "\n",
    "# List of suburbs from the image\n",
    "suburbs_list = [\n",
    "    'Mickleham', 'Yuroke', 'Tarneit', 'Wollert', 'Rockbank',\n",
    "    'Mount Cottrell', 'Cranbourne East', 'Truganina', 'Clyde North', \n",
    "    'Cranbourne West', 'Craigieburn', 'Cobblebank', 'Strathtulloh', \n",
    "    'Lynbrook', 'Lyndhurst', 'Pakenham', 'Cranbourne'\n",
    "]\n",
    "\n",
    "# Find the closest suburbs\n",
    "result_suburbs = []\n",
    "for suburb in suburbs_list:\n",
    "    if suburb in suburbs_gdf['LOC_NAME'].values:\n",
    "        result_suburbs.append(suburb)\n",
    "    else:\n",
    "        suburb_point = suburbs_gdf[suburbs_gdf['LOC_NAME'] == suburb].geometry.iloc[0]\n",
    "        closest_suburb_index = find_closest_suburb(suburb_point, suburbs_gdf)\n",
    "        closest_suburb = suburbs_gdf.loc[closest_suburb_index, 'LOC_NAME']\n",
    "        result_suburbs.append(closest_suburb)\n",
    "\n",
    "# Remove duplicates and take the top 10 from the selected list\n",
    "result_suburbs = list(dict.fromkeys(result_suburbs))[:10]\n",
    "\n",
    "# Filter the main GeoDataFrame to include only the result suburbs\n",
    "filtered_gdf = suburbs_gdf[suburbs_gdf['LOC_NAME'].isin(result_suburbs)]\n",
    "\n",
    "# Sort the filtered GeoDataFrame by livability_score (descending)\n",
    "filtered_gdf = filtered_gdf.sort_values('livability_score', ascending=False).head(10)\n",
    "\n",
    "# Reorder columns to place the Overall_Rank next to LOC_NAME\n",
    "filtered_gdf = filtered_gdf[['LOC_NAME', 'Overall_Rank', 'livability_score', 'train_station_count', \n",
    "                              'fresh_food_count', 'childcare_count', \n",
    "                              'health_count', 'recreation_count', 'geometry']]\n",
    "\n",
    "# Create a Folium map\n",
    "m = folium.Map(location=[-37.8136, 144.9631], zoom_start=10)  # Centered on Melbourne\n",
    "\n",
    "# Create a color map\n",
    "color_map = LinearColormap(colors=['#f9d0c8', '#bd6877', '#4c4557'], \n",
    "                           vmin=0,  # Adjust min for shading\n",
    "                           vmax=filtered_gdf['livability_score'].max())\n",
    "\n",
    "# Add polygons to the map\n",
    "for idx, row in filtered_gdf.iterrows():\n",
    "    folium.GeoJson(\n",
    "        row['geometry'],\n",
    "        style_function=lambda x, score=row['livability_score']: {\n",
    "            'fillColor': color_map(score),\n",
    "            'color': 'black',\n",
    "            'weight': 2,\n",
    "            'fillOpacity': 1,  # Full opacity\n",
    "        },\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add a marker for each suburb at its centroid\n",
    "    centroid = row['geometry'].centroid\n",
    "    folium.Marker(\n",
    "        location=[centroid.y, centroid.x],\n",
    "        icon=folium.Icon(color='blue')\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add color legend to the map\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Prepare the DataFrame for display\n",
    "styled_table = filtered_gdf[['LOC_NAME', 'Overall_Rank', 'livability_score', 'train_station_count', \n",
    "                              'fresh_food_count', 'childcare_count', \n",
    "                              'health_count', 'recreation_count']]\n",
    "\n",
    "# Format the DataFrame for better visibility\n",
    "styled_table['livability_score'] = styled_table['livability_score'].map('{:.2f}'.format)\n",
    "\n",
    "# Create a HTML table string\n",
    "html_table = styled_table.to_html(index=False, \n",
    "                                   classes='table table-striped', \n",
    "                                   border=0, \n",
    "                                   justify='center', \n",
    "                                   col_space=100)\n",
    "\n",
    "# Add custom CSS for styling\n",
    "styled_html = f'''\n",
    "<style>\n",
    ".table {{\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "}}\n",
    ".table th, .table td {{\n",
    "    border: 1px solid black;\n",
    "    padding: 8px;\n",
    "    text-align: center;\n",
    "}}\n",
    ".table th {{\n",
    "    background-color: #0f0f0f;\n",
    "    font-weight: bold;\n",
    "    text-align: center;\n",
    "}}\n",
    "</style>\n",
    "{html_table}\n",
    "'''\n",
    "\n",
    "# Display the styled table\n",
    "display(HTML(styled_html))\n",
    "\n",
    "# Display the map\n",
    "display(m)  # This will render the map in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import LinearColormap\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure suburbs_gdf is a GeoDataFrame\n",
    "if not isinstance(suburbs_gdf, gpd.GeoDataFrame):\n",
    "    suburbs_gdf = gpd.GeoDataFrame(suburbs_gdf, geometry='geometry')\n",
    "\n",
    "# Ensure the geometry column is correctly set\n",
    "if 'geometry' not in suburbs_gdf.columns:\n",
    "    raise KeyError(\"The 'geometry' column is missing from suburbs_gdf.\")\n",
    "\n",
    "# Set the CRS (use the appropriate CRS for your data)\n",
    "suburbs_gdf = suburbs_gdf.set_crs(\"EPSG:4326\")\n",
    "\n",
    "# Rank all suburbs based on livability_score (optional, already exists in previous code)\n",
    "suburbs_gdf['Overall_Rank'] = suburbs_gdf['livability_score'].rank(ascending=False, method='min')\n",
    "\n",
    "# Filter top 10 most livable suburbs\n",
    "top_10_suburbs = suburbs_gdf.sort_values('livability_score', ascending=False).head(10)\n",
    "\n",
    "# Create a Folium map centered on Melbourne\n",
    "m = folium.Map(location=[-37.8136, 144.9631], zoom_start=10)  # Melbourne coordinates\n",
    "\n",
    "# Create a color map based on the range of livability scores\n",
    "color_map = LinearColormap(\n",
    "    colors=['#f9d0c8', '#bd6877', '#4c4557'],  # Define color range\n",
    "    vmin=suburbs_gdf['livability_score'].min(),  # Minimum score\n",
    "    vmax=suburbs_gdf['livability_score'].max()   # Maximum score\n",
    ")\n",
    "\n",
    "# Add polygons to the map for each suburb\n",
    "for idx, row in suburbs_gdf.iterrows():\n",
    "    # Prepare GeoJson feature\n",
    "    geo_json = folium.GeoJson(\n",
    "        row['geometry'],\n",
    "        style_function=lambda x, score=row['livability_score']: {\n",
    "            'fillColor': color_map(score),\n",
    "            'color': 'black',\n",
    "            'weight': 1,\n",
    "            'fillOpacity': 0.7,  # Adjust opacity for the heatmap effect\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Add a tooltip with suburb name and livability score\n",
    "    folium.Popup(f\"{row['LOC_NAME']}: {row['livability_score']:.2f}\").add_to(geo_json)\n",
    "    \n",
    "    geo_json.add_to(m)\n",
    "\n",
    "# Add markers for the top 10 most livable suburbs\n",
    "for idx, row in top_10_suburbs.iterrows():\n",
    "    # Get centroid of the suburb\n",
    "    centroid = row['geometry'].centroid\n",
    "    \n",
    "    # Add a marker at the centroid\n",
    "    folium.Marker(\n",
    "        location=[centroid.y, centroid.x],\n",
    "        popup=f\"Top {int(row['Overall_Rank'])}: {row['LOC_NAME']} (Score: {row['livability_score']:.2f})\",\n",
    "        icon=folium.Icon(color='green', icon='info-sign')  # Green marker for top suburbs\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add the color legend to the map\n",
    "color_map.caption = 'Livability Score of Melbourne Suburbs'\n",
    "color_map.add_to(m)\n",
    "\n",
    "# Display the map\n",
    "display(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
